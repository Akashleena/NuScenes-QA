{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akashleena/NuScenes-QA/blob/main/drivelm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap6KE_ooNd7L",
        "outputId": "0dbae7ee-3f7e-44fb-b62a-9e29c8470b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThK6w_OoStaZ",
        "outputId": "0bb05f3a-dfa0-4ab8-b55a-855f888f4e3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DriveLM_project\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p \"/content/drive/MyDrive/DriveLM_project\"\n",
        "%cd \"/content/drive/MyDrive/DriveLM_project\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM5gP8IOSuA3",
        "outputId": "3933a487-99ed-4db5-c7c1-673caa6bb86a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DriveLM' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/bohdanvodianyk/DriveLM.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "Qij-1gMNT2yp",
        "outputId": "0c87b4df-4f07-4ca5-acf8-a2bec4b4850b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<script>window.scrollTo(0, document.body.scrollHeight);</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3719776403.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<script>window.scrollTo(0, document.body.scrollHeight);</script>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "from IPython.display import HTML, clear_output\n",
        "\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    display(HTML(\"<script>window.scrollTo(0, document.body.scrollHeight);</script>\"))\n",
        "    time.sleep(60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_bni4eGSwgX",
        "outputId": "0615c73f-dd67-4bf4-c391-7f53dae4e4d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DriveLM_project/DriveLM\n"
          ]
        }
      ],
      "source": [
        "%cd DriveLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxOZU_tRSys4",
        "outputId": "798561af-3d4d-4fb1-c842-a37e3957f9f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cpu)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.7.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Collecting peft\n",
            "  Downloading peft-0.17.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cpu)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Downloading peft-0.17.0-py3-none-any.whl (503 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m503.9/503.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: peft\n",
            "Successfully installed peft-0.17.0\n",
            "Collecting pycocotools\n",
            "  Downloading pycocotools-2.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting pycocoevalcap\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (2.0.2)\n",
            "Downloading pycocotools-2.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (477 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m477.3/477.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycocotools, pycocoevalcap\n",
            "Successfully installed pycocoevalcap-1.2 pycocotools-2.0.10\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cpu)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.34.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.7.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers peft\n",
        "!pip install pycocotools pycocoevalcap\n",
        "!pip install accelerate tqdm pandas matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUn_-ZD5dVLw"
      },
      "outputs": [],
      "source": [
        "# Create the correct folder structure first\n",
        "!mkdir -p /content/drive/MyDrive/DriveLM_project/DriveLM/data/nuscenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmDmtP3HS1Es",
        "outputId": "e92a5c94-b742-4d50-80e8-9818ec032630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1DeosPGYeM2gXSChjMODGsQChZyYDmaUz\n",
            "From (redirected): https://drive.google.com/uc?id=1DeosPGYeM2gXSChjMODGsQChZyYDmaUz&confirm=t&uuid=94546035-9e7a-4a6b-b74f-8ffcbe691dfd\n",
            "To: /content/drive/MyDrive/DriveLM_project/DriveLM/drivelm_nus_imgs_train.zip\n",
            "100% 3.48G/3.48G [00:56<00:00, 62.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --fuzzy \"https://drive.google.com/file/d/1DeosPGYeM2gXSChjMODGsQChZyYDmaUz/view\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rYZtBLIUjW9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1IsUQ15bfTz",
        "outputId": "d918207e-02aa-4d36-d6b2-34e80308592e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace /content/drive/MyDrive/DriveLM_project/DriveLM/data/nuscenes/nuscenes/samples/CAM_BACK/n008-2018-08-01-15-52-19-0400__CAM_BACK__1533153528287558.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "y\n",
            "All\n"
          ]
        }
      ],
      "source": [
        "!unzip -q /content/drive/MyDrive/DriveLM_project/DriveLM/drivelm_nus_imgs_train.zip \\\n",
        "    -d /content/drive/MyDrive/DriveLM_project/DriveLM/data/nuscenes/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBByg8NTexzJ"
      },
      "outputs": [],
      "source": [
        "# Move the samples folder up one level\n",
        "!mv /content/drive/MyDrive/DriveLM_project/DriveLM/data/nuscenes/nuscenes/samples \\\n",
        "    /content/drive/MyDrive/DriveLM_project/DriveLM/data/nuscenes/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0jzWJLce4Wd"
      },
      "outputs": [],
      "source": [
        "# Remove the now-empty extra nuscenes folder\n",
        "!rm -r /content/drive/MyDrive/DriveLM_project/DriveLM/data/nuscenes/nuscenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHI95WCmfDUZ",
        "outputId": "244908ff-27d5-4457-febc-10e592b08dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1LK7pYHytv64neN1626u6eTQBy1Uf4IQH\n",
            "From (redirected): https://drive.google.com/uc?id=1LK7pYHytv64neN1626u6eTQBy1Uf4IQH&confirm=t&uuid=3453508e-4d92-4aa3-9ebe-f9e4af52e291\n",
            "To: /content/drive/MyDrive/DriveLM_project/DriveLM/v1_0_train_nus.json\n",
            "100% 193M/193M [00:02<00:00, 90.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --fuzzy \"https://drive.google.com/file/d/1LK7pYHytv64neN1626u6eTQBy1Uf4IQH/view\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhHdzPxgfNxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b8d6d57-740e-4abf-d101-90449f0744ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace /content/drive/MyDrive/DriveLM_project/DriveLM/data/nuscenes/nuscenes/samples/CAM_BACK/n008-2018-08-01-15-52-19-0400__CAM_BACK__1533153528287558.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: All\n",
            "A\n",
            "A\n",
            "A\n"
          ]
        }
      ],
      "source": [
        "# 1Ô∏è‚É£ Make the QA_dataset_nus folder\n",
        "!mkdir -p /content/drive/MyDrive/DriveLM_project/DriveLM/data/QA_dataset_nus\n",
        "\n",
        "# 2Ô∏è‚É£ Move your downloaded v1_0_train_nus.json into it\n",
        "!mv /content/drive/MyDrive/DriveLM_project/DriveLM/v1_0_train_nus.json \\\n",
        "    /content/drive/MyDrive/DriveLM_project/DriveLM/data/QA_dataset_nus/\n",
        "\n",
        "# 3Ô∏è‚É£ Unzip your nuScenes subset into correct location\n",
        "!unzip -q /content/drive/MyDrive/DriveLM_project/DriveLM/drivelm_nus_imgs_train.zip \\\n",
        "    -d /content/drive/MyDrive/DriveLM_project/DriveLM/data/nuscenes/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evIW3mOdgwUy",
        "outputId": "65663eaf-0311-4827-aac2-4d52077998c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Split complete!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load QA JSON\n",
        "with open(\"/content/drive/MyDrive/DriveLM_project/DriveLM/data/QA_dataset_nus/v1_0_train_nus.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Split: 70% train, 15% val, 15% test\n",
        "train_data, temp = train_test_split(list(data.items()), test_size=0.3, random_state=42)\n",
        "val_data, test_data = train_test_split(temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save splits\n",
        "def save_json(filename, dataset):\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(dict(dataset), f, indent=2)\n",
        "\n",
        "save_json(\"/content/drive/MyDrive/DriveLM_project/DriveLM/data/QA_dataset_nus/train.json\", train_data)\n",
        "save_json(\"/content/drive/MyDrive/DriveLM_project/DriveLM/data/QA_dataset_nus/val.json\", val_data)\n",
        "save_json(\"/content/drive/MyDrive/DriveLM_project/DriveLM/data/QA_dataset_nus/test.json\", test_data)\n",
        "\n",
        "print(\"‚úÖ Split complete!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd723DU2p7t4",
        "outputId": "7a63d3c0-62ee-475d-e834-defa5e0348db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved: /content/drive/MyDrive/DriveLM_project/DriveLM/data/multi_frame/multi_frame_train.json (2864 entries)\n",
            "‚úÖ Saved: /content/drive/MyDrive/DriveLM_project/DriveLM/data/multi_frame/multi_frame_val.json (608 entries)\n",
            "‚úÖ Saved: /content/drive/MyDrive/DriveLM_project/DriveLM/data/multi_frame/multi_frame_test.json (600 entries)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "qa_dir = \"/content/drive/MyDrive/DriveLM_project/DriveLM/data/QA_dataset_nus\"\n",
        "multi_frame_dir = \"/content/drive/MyDrive/DriveLM_project/DriveLM/data/multi_frame\"\n",
        "os.makedirs(multi_frame_dir, exist_ok=True)\n",
        "\n",
        "# Function to convert QA format to Vlad's multi_frame format\n",
        "def convert_to_multiframe(input_path, output_path):\n",
        "    with open(input_path, \"r\") as f:\n",
        "        qa_data = json.load(f)\n",
        "\n",
        "    multi_frame_data = []\n",
        "    for scene_id, scene_data in qa_data.items():\n",
        "        for frame_id, frame_info in scene_data.get(\"key_frames\", {}).items():\n",
        "            entry = {\n",
        "                \"scene_id\": scene_id,\n",
        "                \"frame_id\": frame_id,\n",
        "                \"image_paths\": frame_info.get(\"image_paths\", {}),\n",
        "                \"QA\": frame_info.get(\"QA\", {})\n",
        "            }\n",
        "            multi_frame_data.append(entry)\n",
        "\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(multi_frame_data, f, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Saved: {output_path} ({len(multi_frame_data)} entries)\")\n",
        "\n",
        "# Convert all three splits\n",
        "convert_to_multiframe(\n",
        "    os.path.join(qa_dir, \"train.json\"),\n",
        "    os.path.join(multi_frame_dir, \"multi_frame_train.json\")\n",
        ")\n",
        "convert_to_multiframe(\n",
        "    os.path.join(qa_dir, \"val.json\"),\n",
        "    os.path.join(multi_frame_dir, \"multi_frame_val.json\")\n",
        ")\n",
        "convert_to_multiframe(\n",
        "    os.path.join(qa_dir, \"test.json\"),\n",
        "    os.path.join(multi_frame_dir, \"multi_frame_test.json\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93NdkXxTqYrS",
        "outputId": "734fb0f4-671f-4317-8dab-a84b21c4f150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created (or already exists): /content/drive/MyDrive/DriveLM_project/DriveLM/multi_frame_results\n"
          ]
        }
      ],
      "source": [
        "# create the directory multi_frame_results\n",
        "import os\n",
        "\n",
        "base_results_dir = \"/content/drive/MyDrive/DriveLM_project/DriveLM/multi_frame_results\"\n",
        "os.makedirs(base_results_dir, exist_ok=True)\n",
        "print(f\"‚úÖ Created (or already exists): {base_results_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmuQ_6i2tUvo"
      },
      "source": [
        "Preprocessing for QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60L40cPytXLI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def preprocess_driveLM_json(input_json_path, output_json_path):\n",
        "    # Load DriveLM's original JSON\n",
        "    with open(input_json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    processed = []\n",
        "\n",
        "    # Loop through scenes\n",
        "    for scene_token, scene_data in data.items():\n",
        "        key_frames = scene_data.get(\"key_frames\", {})\n",
        "        # Loop through key frames\n",
        "        for frame_token, frame_data in key_frames.items():\n",
        "            qa = frame_data.get(\"QA\", {})\n",
        "            img_paths = frame_data.get(\"image_paths\", {})\n",
        "\n",
        "            # Append tuple exactly as Vlad's code expects\n",
        "            processed.append((qa, img_paths))\n",
        "\n",
        "    # Save processed JSON\n",
        "    with open(output_json_path, \"w\") as f:\n",
        "        json.dump(processed, f)\n",
        "\n",
        "    print(f\"‚úÖ Saved {len(processed)} entries to {output_json_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV81BTr_tec0",
        "outputId": "d916cb26-c183-408f-a638-5d773087ea74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved 2864 entries to /content/drive/MyDrive/DriveLM_project/DriveLM/data/multi_frame/multi_frame_train.json\n",
            "‚úÖ Saved 608 entries to /content/drive/MyDrive/DriveLM_project/DriveLM/data/multi_frame/multi_frame_val.json\n",
            "‚úÖ Saved 600 entries to /content/drive/MyDrive/DriveLM_project/DriveLM/data/multi_frame/multi_frame_test.json\n"
          ]
        }
      ],
      "source": [
        "base_dir = \"/content/drive/MyDrive/DriveLM_project/DriveLM/data\"\n",
        "\n",
        "# Train\n",
        "preprocess_driveLM_json(\n",
        "    f\"{base_dir}/QA_dataset_nus/train.json\",\n",
        "    f\"{base_dir}/multi_frame/multi_frame_train.json\"\n",
        ")\n",
        "\n",
        "# Val\n",
        "preprocess_driveLM_json(\n",
        "    f\"{base_dir}/QA_dataset_nus/val.json\",\n",
        "    f\"{base_dir}/multi_frame/multi_frame_val.json\"\n",
        ")\n",
        "\n",
        "# Test\n",
        "preprocess_driveLM_json(\n",
        "    f\"{base_dir}/QA_dataset_nus/test.json\",\n",
        "    f\"{base_dir}/multi_frame/multi_frame_test.json\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zTMMKSAtls-",
        "outputId": "26f8b7ec-81e0-4e0f-9e45-4b1e7d8950b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2864\n",
            "<class 'list'>\n",
            "dict_keys(['perception', 'prediction', 'planning', 'behavior'])\n",
            "dict_keys(['CAM_FRONT', 'CAM_FRONT_LEFT', 'CAM_FRONT_RIGHT', 'CAM_BACK', 'CAM_BACK_LEFT', 'CAM_BACK_RIGHT'])\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "with open(f\"{base_dir}/multi_frame/multi_frame_train.json\", \"r\") as f:\n",
        "    processed_data = json.load(f)\n",
        "\n",
        "print(len(processed_data))\n",
        "print(type(processed_data[0]))\n",
        "print(processed_data[0][0].keys())  # QA keys\n",
        "print(processed_data[0][1].keys())  # camera views"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjv9myvWvQgX",
        "outputId": "39a1ed93-86b3-4262-fcc2-96f6a61d7be4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Converted train: 265484 entries\n",
            "‚úÖ Converted val: 56554 entries\n",
            "‚úÖ Converted test: 55945 entries\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/DriveLM_project/DriveLM/data\"\n",
        "\n",
        "# ---------------- TRAIN ----------------\n",
        "with open(f\"{base_dir}/multi_frame/multi_frame_train.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "converted = []\n",
        "for qa_block, cam_block in data:\n",
        "    for category in [\"perception\", \"prediction\", \"planning\", \"behavior\"]:\n",
        "        if category in qa_block:\n",
        "            for qa_pair in qa_block[category]:\n",
        "                converted.append([qa_pair, cam_block])\n",
        "\n",
        "with open(f\"{base_dir}/multi_frame/multi_frame_train.json\", \"w\") as f:\n",
        "    json.dump(converted, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Converted train: {len(converted)} entries\")\n",
        "\n",
        "\n",
        "# ---------------- VAL ----------------\n",
        "with open(f\"{base_dir}/multi_frame/multi_frame_val.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "converted = []\n",
        "for qa_block, cam_block in data:\n",
        "    for category in [\"perception\", \"prediction\", \"planning\", \"behavior\"]:\n",
        "        if category in qa_block:\n",
        "            for qa_pair in qa_block[category]:\n",
        "                converted.append([qa_pair, cam_block])\n",
        "\n",
        "with open(f\"{base_dir}/multi_frame/multi_frame_val.json\", \"w\") as f:\n",
        "    json.dump(converted, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Converted val: {len(converted)} entries\")\n",
        "\n",
        "\n",
        "# ---------------- TEST ----------------\n",
        "with open(f\"{base_dir}/multi_frame/multi_frame_test.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "converted = []\n",
        "for qa_block, cam_block in data:\n",
        "    for category in [\"perception\", \"prediction\", \"planning\", \"behavior\"]:\n",
        "        if category in qa_block:\n",
        "            for qa_pair in qa_block[category]:\n",
        "                converted.append([qa_pair, cam_block])\n",
        "\n",
        "with open(f\"{base_dir}/multi_frame/multi_frame_test.json\", \"w\") as f:\n",
        "    json.dump(converted, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Converted test: {len(converted)} entries\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA7q-PmByk36",
        "outputId": "9e441dbc-1463-450d-96d2-c1c6bc343949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ TRAIN ‚Äî total entries: 265484\n",
            "\n",
            "--- QA block ---\n",
            "Q: What are objects to the back of the ego car?\n",
            "A: There are many cars and one truck behind the ego car.\n",
            "C: None\n",
            "con_up: None\n",
            "con_down: None\n",
            "cluster: None\n",
            "layer: None\n",
            "\n",
            "--- Camera paths ---\n",
            "CAM_FRONT: ../nuscenes/samples/CAM_FRONT/n008-2018-07-27-12-07-38-0400__CAM_FRONT__1532707722612404.jpg\n",
            "CAM_FRONT_LEFT: ../nuscenes/samples/CAM_FRONT_LEFT/n008-2018-07-27-12-07-38-0400__CAM_FRONT_LEFT__1532707722604799.jpg\n",
            "CAM_FRONT_RIGHT: ../nuscenes/samples/CAM_FRONT_RIGHT/n008-2018-07-27-12-07-38-0400__CAM_FRONT_RIGHT__1532707722620482.jpg\n",
            "CAM_BACK: ../nuscenes/samples/CAM_BACK/n008-2018-07-27-12-07-38-0400__CAM_BACK__1532707722637558.jpg\n",
            "CAM_BACK_LEFT: ../nuscenes/samples/CAM_BACK_LEFT/n008-2018-07-27-12-07-38-0400__CAM_BACK_LEFT__1532707722647405.jpg\n",
            "CAM_BACK_RIGHT: ../nuscenes/samples/CAM_BACK_RIGHT/n008-2018-07-27-12-07-38-0400__CAM_BACK_RIGHT__1532707722628125.jpg\n",
            "============================================================\n",
            "üìÇ VAL ‚Äî total entries: 56554\n",
            "\n",
            "--- QA block ---\n",
            "Q: What are objects to the back left of the ego car?\n",
            "A: There are many traffic cones, two pedestrians, two bicycles, many cars, and one truck to the back left of the ego car.\n",
            "C: None\n",
            "con_up: None\n",
            "con_down: None\n",
            "cluster: None\n",
            "layer: None\n",
            "\n",
            "--- Camera paths ---\n",
            "CAM_FRONT: ../nuscenes/samples/CAM_FRONT/n008-2018-09-18-14-54-39-0400__CAM_FRONT__1537297631912404.jpg\n",
            "CAM_FRONT_LEFT: ../nuscenes/samples/CAM_FRONT_LEFT/n008-2018-09-18-14-54-39-0400__CAM_FRONT_LEFT__1537297631904799.jpg\n",
            "CAM_FRONT_RIGHT: ../nuscenes/samples/CAM_FRONT_RIGHT/n008-2018-09-18-14-54-39-0400__CAM_FRONT_RIGHT__1537297631920482.jpg\n",
            "CAM_BACK: ../nuscenes/samples/CAM_BACK/n008-2018-09-18-14-54-39-0400__CAM_BACK__1537297631937558.jpg\n",
            "CAM_BACK_LEFT: ../nuscenes/samples/CAM_BACK_LEFT/n008-2018-09-18-14-54-39-0400__CAM_BACK_LEFT__1537297631947405.jpg\n",
            "CAM_BACK_RIGHT: ../nuscenes/samples/CAM_BACK_RIGHT/n008-2018-09-18-14-54-39-0400__CAM_BACK_RIGHT__1537297631928113.jpg\n",
            "============================================================\n",
            "üìÇ TEST ‚Äî total entries: 55945\n",
            "\n",
            "--- QA block ---\n",
            "Q: What are objects to the front of the ego car?\n",
            "A: There are two buses in front of the ego car.\n",
            "C: None\n",
            "con_up: None\n",
            "con_down: None\n",
            "cluster: None\n",
            "layer: None\n",
            "\n",
            "--- Camera paths ---\n",
            "CAM_FRONT: ../nuscenes/samples/CAM_FRONT/n015-2018-11-14-18-57-54+0800__CAM_FRONT__1542193187862460.jpg\n",
            "CAM_FRONT_LEFT: ../nuscenes/samples/CAM_FRONT_LEFT/n015-2018-11-14-18-57-54+0800__CAM_FRONT_LEFT__1542193187854928.jpg\n",
            "CAM_FRONT_RIGHT: ../nuscenes/samples/CAM_FRONT_RIGHT/n015-2018-11-14-18-57-54+0800__CAM_FRONT_RIGHT__1542193187870339.jpg\n",
            "CAM_BACK: ../nuscenes/samples/CAM_BACK/n015-2018-11-14-18-57-54+0800__CAM_BACK__1542193187887525.jpg\n",
            "CAM_BACK_LEFT: ../nuscenes/samples/CAM_BACK_LEFT/n015-2018-11-14-18-57-54+0800__CAM_BACK_LEFT__1542193187897423.jpg\n",
            "CAM_BACK_RIGHT: ../nuscenes/samples/CAM_BACK_RIGHT/n015-2018-11-14-18-57-54+0800__CAM_BACK_RIGHT__1542193187877893.jpg\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/DriveLM_project/DriveLM/data\"\n",
        "\n",
        "def show_one_example(split):\n",
        "    path = f\"{base_dir}/multi_frame/multi_frame_{split}.json\"\n",
        "    with open(path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(f\"üìÇ {split.upper()} ‚Äî total entries: {len(data)}\")\n",
        "    example = data[0]  # first item\n",
        "    qa_block, cam_block = example\n",
        "\n",
        "    print(\"\\n--- QA block ---\")\n",
        "    for k, v in qa_block.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "    print(\"\\n--- Camera paths ---\")\n",
        "    for k, v in cam_block.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# Show one from each split\n",
        "show_one_example(\"train\")\n",
        "show_one_example(\"val\")\n",
        "show_one_example(\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x_NaRHzzb4O",
        "outputId": "752cfa22-a94e-48bc-fa0a-c2666a7c6bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fixed paths in /content/drive/MyDrive/DriveLM_project/DriveLM/data/multi_frame/multi_frame_train.json\n",
            "‚úÖ Fixed paths in /content/drive/MyDrive/DriveLM_project/DriveLM/data/multi_frame/multi_frame_val.json\n",
            "‚úÖ Fixed paths in /content/drive/MyDrive/DriveLM_project/DriveLM/data/multi_frame/multi_frame_test.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/DriveLM_project/DriveLM/data\"\n",
        "abs_base = os.path.join(base_dir, \"nuscenes\")  # where samples/ is located\n",
        "\n",
        "def fix_paths(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    fixed = []\n",
        "    for qa_block, cam_block in data:\n",
        "        fixed_cam_block = {}\n",
        "        for cam, rel_path in cam_block.items():\n",
        "            # Remove \"../nuscenes/\" if present\n",
        "            clean_rel = rel_path.replace(\"../nuscenes/\", \"\")\n",
        "            # Make absolute path\n",
        "            abs_path = os.path.join(abs_base, clean_rel)\n",
        "            fixed_cam_block[cam] = abs_path\n",
        "        fixed.append([qa_block, fixed_cam_block])\n",
        "\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(fixed, f, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Fixed paths in {json_path}\")\n",
        "\n",
        "# Run for all splits\n",
        "for split in [\"multi_frame_train.json\", \"multi_frame_val.json\", \"multi_frame_test.json\"]:\n",
        "    fix_paths(os.path.join(base_dir, \"multi_frame\", split))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFkY_HTqhj_3"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fUlvdGChcAT"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision.io import read_image\n",
        "from transformers import T5Tokenizer\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer, GPT2LMHeadModel, GPT2TokenizerFast\n",
        "from torchvision.models import vit_b_32\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import argparse\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgPGHGA_hnPD"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "TEXT_MAX_LENGTH = 463\n",
        "VIT_HIDDEN_STATE = 768\n",
        "VIT_SEQ_LENGTH = 49\n",
        "GPT_N_EMBED = 1024\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "\n",
        "    print(\n",
        "        f\"Trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "class DriveVLM(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Make tokenizer and text model\n",
        "        if config.lm == 'T5':\n",
        "            self.model = T5ForConditionalGeneration.from_pretrained('google-t5/t5-base')\n",
        "            hidden_size = self.model.config.d_model\n",
        "        else:\n",
        "            self.model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "            hidden_size = GPT_N_EMBED\n",
        "\n",
        "\n",
        "        # Only if we are perfoming LoRA finetuning on T5\n",
        "        if config.lora:\n",
        "\n",
        "            if config.lm == 'T5':\n",
        "              tm = ['q', 'v']\n",
        "            else:\n",
        "              tm = ['c_attn']\n",
        "\n",
        "            # Create LoRA model\n",
        "            lora_config = LoraConfig(\n",
        "                r=config.lora_dim,\n",
        "                lora_alpha=config.lora_alpha,\n",
        "                lora_dropout=config.lora_dropout,\n",
        "                bias='none',\n",
        "                target_modules=tm\n",
        "            )\n",
        "            self.model = get_peft_model(self.model, lora_config)\n",
        "\n",
        "        # Freeze model weights if needed\n",
        "        if config.freeze_lm:\n",
        "\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        print('Trainable Parameters for LM model:')\n",
        "        print_trainable_parameters(self.model)\n",
        "\n",
        "        # Create instance for multi-view processor\n",
        "        self.mvp = self.MultiViewProcessor(config.gpa_hidden_size, hidden_size, config.lm, freeze=True)\n",
        "\n",
        "    class MultiViewProcessor(nn.Module):\n",
        "\n",
        "        def __init__(self, gpa_hidden_size, hidden_size, lm, freeze=False):\n",
        "\n",
        "            super().__init__()\n",
        "\n",
        "            # Use ViT for image embeddings\n",
        "            self.img_model = vit_b_32(weights='DEFAULT')\n",
        "            self.lm = lm\n",
        "\n",
        "            # Modal embedding to distinguish between image and text\n",
        "            self.modal_embeddings = nn.Embedding(2, hidden_size)\n",
        "            self.modal_embeddings.weight.data.normal_(mean=0.0, std=0.02)\n",
        "\n",
        "            # If we are freezing the CLIP embeddings\n",
        "            if freeze:\n",
        "                for param in self.img_model.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "            # Set matrices based on MIVC paper\n",
        "            self.w = nn.Linear(in_features=gpa_hidden_size, out_features=1)\n",
        "            self.Z = nn.Sequential(\n",
        "                nn.Linear(in_features=VIT_HIDDEN_STATE * VIT_SEQ_LENGTH, out_features=gpa_hidden_size, bias=False),\n",
        "                nn.Tanh()\n",
        "            )\n",
        "            self.G = nn.Sequential(\n",
        "                nn.Linear(in_features=VIT_HIDDEN_STATE * VIT_SEQ_LENGTH, out_features=gpa_hidden_size, bias=False),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "            if self.lm != 'T5':\n",
        "              self.img_projection_layer = nn.Linear(in_features=VIT_HIDDEN_STATE, out_features=hidden_size)\n",
        "\n",
        "\n",
        "        def gpa(self, img_embeddings):\n",
        "\n",
        "            \"\"\"\"\n",
        "            Calculates the gated-pooling attention score for the image embeddings\n",
        "            :param img_embeddings: (6x768) dimensional\n",
        "            :return single embedding of size (768,)\n",
        "            \"\"\"\n",
        "\n",
        "            # Get weights for gated pooling attention\n",
        "            gpa_weights = torch.softmax(self.w(self.Z(img_embeddings) * self.G(img_embeddings)), dim=0  )\n",
        "\n",
        "            # Take a linear combination of all the image embeddings\n",
        "            fused_embeddings = torch.sum(gpa_weights * img_embeddings, dim=0)\n",
        "\n",
        "            return fused_embeddings\n",
        "\n",
        "        def get_img_embedding(self, imgs):\n",
        "\n",
        "            N = imgs.shape[0]\n",
        "\n",
        "            # Process into patches (N x 6 x 49 x H)\n",
        "            merged_embedding = torch.stack([self.img_model._process_input(img) for img in imgs], dim=0)\n",
        "\n",
        "            # Concatenate the batch class tokens -> (N, 6, 50, H)\n",
        "            batch_class_tokens = self.img_model.class_token.expand(merged_embedding.shape[1], -1, -1).repeat(N, 1, 1, 1)\n",
        "            merged_embedding = torch.cat([batch_class_tokens, merged_embedding], dim=2)\n",
        "\n",
        "            # Add positional embeddings and remove class token -> (N, 6, 49, H)\n",
        "            merged_embedding += self.img_model.encoder.pos_embedding.repeat(N, 1, 1, 1)\n",
        "            merged_embedding = merged_embedding[:, :, 1:]\n",
        "\n",
        "            # Get merged embedding and reshape to 2D embedding -> (N, 1, 49, H)\n",
        "            merged_embedding = torch.stack([self.gpa(embedding.flatten(start_dim=1)).reshape(VIT_SEQ_LENGTH,\n",
        "                                            VIT_HIDDEN_STATE) for embedding in merged_embedding], dim=0)\n",
        "\n",
        "            # Project to VL dimension -> (1, 49, H) (H is 512 for t5-small, 768 for t5-base)\n",
        "            if self.lm != 'T5':\n",
        "              merged_embedding = self.img_projection_layer(merged_embedding)\n",
        "\n",
        "            # Add modal type embedding to merged embedding\n",
        "            merged_embedding += self.modal_embeddings(\n",
        "                torch.ones((1, merged_embedding.shape[1]), dtype=torch.int, device=device))\n",
        "\n",
        "            return merged_embedding\n",
        "\n",
        "        def forward(self, text_enc, imgs, text_model):\n",
        "\n",
        "            # Get the image embeddings (N x 1 x 49 x H)\n",
        "            imgs_embedding = self.get_img_embedding(imgs)\n",
        "\n",
        "            # Get the text embeddings (N x S x H)\n",
        "            text_embeddings = text_model.get_input_embeddings()(text_enc)\n",
        "\n",
        "            # Add modal embeddings to text\n",
        "            text_embeddings += self.modal_embeddings(torch.zeros((1, text_embeddings.shape[1]), dtype=torch.int,\n",
        "                                                                 device=device))\n",
        "\n",
        "            # Concatenate embeddings -> (1 x S x 512)\n",
        "            merged_embedding = torch.cat([text_embeddings, imgs_embedding], dim=1)\n",
        "\n",
        "            return merged_embedding\n",
        "\n",
        "    def forward(self, text_enc, imgs, labels=None):\n",
        "\n",
        "        # Get the merged embeddings\n",
        "        merged_embedding = self.mvp(text_enc, imgs, self.model)\n",
        "\n",
        "        # If training include the labels\n",
        "        return self.model(inputs_embeds=merged_embedding, labels=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ894wWQhwHZ"
      },
      "outputs": [],
      "source": [
        "class MultiFrameDataset(Dataset):\n",
        "\n",
        "    def __init__(self, input_file, tokenizer, transform=None):\n",
        "        with open(input_file) as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the question and answer at the idx\n",
        "        qa, img_path = self.data[idx]\n",
        "        img_path = [os.path.join('DriveLM', p)\n",
        "                    for p in list(img_path.values())]\n",
        "\n",
        "        q_text, a_text = qa['Q'], qa['A']\n",
        "        q_text = f\"Question: {q_text} Answer:\"\n",
        "\n",
        "        # Concatenate images into a single tensor\n",
        "        imgs = [self.transform(read_image(p).float()).to(device) for p in img_path]\n",
        "        imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "        return q_text, imgs, a_text\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        q_texts, imgs, a_texts = zip(*batch)\n",
        "        imgs = torch.stack(list(imgs), dim=0)\n",
        "\n",
        "        encodings = self.tokenizer(q_texts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "        labels = self.tokenizer(a_texts, padding=True, return_tensors='pt').input_ids.to(device)\n",
        "\n",
        "        return encodings, imgs, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg8pth7VhydU"
      },
      "outputs": [],
      "source": [
        "Config = namedtuple('Instance', ['batch_size', 'learning_rate',\n",
        "                                 'weight_decay', 'num_workers',\n",
        "                                 'epochs', 'custom_train', 'gpa_hidden_size',\n",
        "                                 'lora', 'lora_dim', 'lora_alpha', 'lora_dropout',\n",
        "                                 'load_checkpoint', 'file_checkpoint', 'checkpoint_frequency',\n",
        "                                 'freeze_lm', 'lm'])\n",
        "\n",
        "# We recommend keep all hyperparameters the same besides the following\n",
        "# file_checkpoint -> Checkpoint folder stored in multi_frame_results folder\n",
        "# load_checkpoint -> Use if you want to resume training from a checkpoint stored in multi_frame_results\n",
        "config = Config(\n",
        "    batch_size = 4,\n",
        "    learning_rate = 1e-4,\n",
        "    weight_decay = 0.05,\n",
        "    num_workers = 0,\n",
        "    epochs = 6,\n",
        "    custom_train = True,\n",
        "    gpa_hidden_size = 128,\n",
        "    lora = False,\n",
        "    lora_dim = 64,\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.05,\n",
        "    load_checkpoint = False,\n",
        "    file_checkpoint = '20240301-053312',\n",
        "    checkpoint_frequency = 15000,\n",
        "    freeze_lm = True,\n",
        "    lm = 'T5'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COEYjtoPN6Vy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYjkMRaiiDEB",
        "outputId": "cba66c55-a615-4aca-b8bd-7103692ec4f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DriveLM_project/DriveLM\n"
          ]
        }
      ],
      "source": [
        "# Set working directory\n",
        "%cd '/content/drive/MyDrive/DriveLM_project/DriveLM'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def save_model(state_dict, model_name):\n",
        "    \"\"\"Save only model weights (state_dict) to save space.\"\"\"\n",
        "    save_path = os.path.join(\n",
        "        '/content/drive/MyDrive/DriveLM_project/DriveLM',\n",
        "        'multi_frame_results', timestr, model_name + '.pth'\n",
        "    )\n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f\"üíæ Saved checkpoint: {save_path}\")\n",
        "\n",
        "def load_model(model, checkpoint_name):\n",
        "    \"\"\"Load model weights into an existing model.\"\"\"\n",
        "    load_path = os.path.join(\n",
        "        '/content/drive/MyDrive/DriveLM_project/DriveLM',\n",
        "        'multi_frame_results', timestr, checkpoint_name + '.pth'\n",
        "    )\n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    print(f\"‚úÖ Loaded checkpoint: {load_path}\")\n",
        "    return model\n",
        "\n",
        "def val_model(dloader, val_model):\n",
        "    val_model.eval()\n",
        "    val_loss = 0\n",
        "    for idx, (inputs, imgs, labels) in tqdm(enumerate(dloader), total=len(dloader)):\n",
        "        outputs = val_model(inputs, imgs, labels)\n",
        "        val_loss += outputs.loss.item()\n",
        "    return val_loss / len(val_dataloader)\n",
        "\n",
        "def save_stats(train_loss, val_loss, epochs, lr):\n",
        "    stats_dict = {\n",
        "        'losses': losses,\n",
        "        'val losses': val_losses,\n",
        "        'min train loss': train_loss,\n",
        "        'min val loss': val_loss,\n",
        "        'epochs': epochs,\n",
        "        'learning rate': lr,\n",
        "        'LM': 'T5-Base',\n",
        "        'Image Embedding': 'Patch'\n",
        "    }\n",
        "    with open(os.path.join('/content/drive/MyDrive/DriveLM_project/DriveLM',\n",
        "                           'multi_frame_results', timestr, 'stats.json'), 'w') as f:\n",
        "        json.dump(stats_dict, f)\n",
        "\n",
        "def plot_loss(training_loss, val_loss):\n",
        "    num_epochs = len(training_loss)\n",
        "    plt.plot(range(1, num_epochs + 1), training_loss, label='Training Loss')\n",
        "    plt.plot(range(1, num_epochs + 1), val_loss, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Num epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join('/content/drive/MyDrive/DriveLM_project/DriveLM',\n",
        "                             'multi_frame_results', timestr, 'loss.png'))\n",
        "\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-km1YyvoKmo6"
      },
      "outputs": [],
      "source": [
        " ========================\n",
        "# TRAINING LOOP\n",
        "# ========================\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "def custom_train(train_loss, val_loss, best_model, epochs, learning_rate):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    checkpoint_every_steps = 5000  # mid-epoch save frequency\n",
        "\n",
        "    for epoch in range(epochs, config.epochs):\n",
        "        print(f'-------------------- EPOCH {epoch} ---------------------')\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for step, (inputs, imgs, labels) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(inputs, imgs, labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if step % config.checkpoint_frequency == 0:\n",
        "                print(f'\\nLoss: {loss.item()}')\n",
        "\n",
        "            # backprop with AMP\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # mid-epoch checkpoint save\n",
        "            if step > 0 and step % checkpoint_every_steps == 0:\n",
        "                ckpt_name = f\"epoch{epoch}_step{step}\"\n",
        "                save_model(model.state_dict(), ckpt_name)\n",
        "\n",
        "        # end of epoch logging\n",
        "        epoch_train_loss = epoch_loss / len(train_dataloader)\n",
        "        losses.append(epoch_train_loss)\n",
        "        epoch_val_loss = val_model(val_dataloader, model)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "\n",
        "        # best model tracking\n",
        "        if not val_loss or epoch_val_loss < val_loss:\n",
        "            val_loss = epoch_val_loss\n",
        "            best_model = deepcopy(model.state_dict())\n",
        "        if not train_loss or epoch_train_loss < train_loss:\n",
        "            train_loss = epoch_train_loss\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f'Training Loss: {epoch_train_loss} | Validation Loss: {epoch_val_loss}')\n",
        "\n",
        "        # save end-of-epoch checkpoint\n",
        "        save_model(best_model, 'latest_model')\n",
        "        epochs += 1\n",
        "        save_stats(train_loss, val_loss, epochs, scheduler.get_last_lr()[0])\n",
        "\n",
        "    plot_loss(losses, val_losses)\n",
        "    return train_loss, val_loss\n",
        "\n",
        "# ========================\n",
        "# EXPERIMENT LOGGING\n",
        "# ========================\n",
        "def save_experiment(statistics):\n",
        "    trial_dict = {\n",
        "        'Model name': [timestr],\n",
        "        'Learning rate': [config.learning_rate],\n",
        "        'Weight decay': [config.weight_decay],\n",
        "        'Batch size': [config.batch_size],\n",
        "        'Epochs': [config.epochs],\n",
        "        'LoRA finetuning': [config.lora],\n",
        "        'GPA Hidden Size': [config.gpa_hidden_size],\n",
        "        'LoRA Dimension': [config.lora_dim],\n",
        "        'LoRA Alpha': [config.lora_alpha],\n",
        "        'LoRA Dropout': [config.lora_dropout],\n",
        "        'Freeze T5': [config.freeze_lm],\n",
        "        'Min Training Loss': [statistics[0]],\n",
        "        'Min Validation Loss': [statistics[1]],\n",
        "        'Min Testing Loss': [statistics[2]],\n",
        "    }\n",
        "    pd.DataFrame(trial_dict).to_csv(\n",
        "        os.path.join('/content/drive/MyDrive/DriveLM_project/DriveLM',\n",
        "                     'multi_frame_results', timestr, 'multi_frame_results.csv'),\n",
        "        index=False, header=True\n",
        "    )\n",
        "\n",
        "# ========================\n",
        "# MAIN EXECUTION\n",
        "# ========================\n",
        "if __name__ == '__main__':\n",
        "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    losses, val_losses = [], []\n",
        "    min_train_loss, min_val_loss, best_model, epochs_ran = None, None, None, 0\n",
        "\n",
        "    model = DriveVLM(config).to(device)\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    # Tokenizer\n",
        "    if config.lm == 'T5':\n",
        "        processor = T5Tokenizer.from_pretrained('google-t5/t5-base')\n",
        "        processor.add_tokens('<')\n",
        "    else:\n",
        "        processor = GPT2TokenizerFast.from_pretrained('gpt2-medium')\n",
        "        processor.pad_token = processor.eos_token\n",
        "\n",
        "    # datasets\n",
        "    train_dset = MultiFrameDataset(\n",
        "        os.path.join('/content/drive/MyDrive/DriveLM_project/DriveLM', 'data', 'multi_frame', 'multi_frame_train.json'),\n",
        "        tokenizer=processor,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))\n",
        "        ])\n",
        "    )\n",
        "    val_dset = MultiFrameDataset(\n",
        "        os.path.join('/content/drive/MyDrive/DriveLM_project/DriveLM', 'data', 'multi_frame', 'multi_frame_val.json'),\n",
        "        tokenizer=processor,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))\n",
        "        ])\n",
        "    )\n",
        "    test_dset = MultiFrameDataset(\n",
        "        os.path.join('/content/drive/MyDrive/DriveLM_project/DriveLM', 'data', 'multi_frame', 'multi_frame_test.json'),\n",
        "        tokenizer=processor,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    # dataloaders\n",
        "    train_dataloader = DataLoader(train_dset, shuffle=True, batch_size=config.batch_size,\n",
        "                                  num_workers=config.num_workers, collate_fn=train_dset.collate_fn)\n",
        "    val_dataloader = DataLoader(val_dset, shuffle=True, batch_size=config.batch_size,\n",
        "                                num_workers=config.num_workers, collate_fn=train_dset.collate_fn)\n",
        "    test_dataloader = DataLoader(test_dset, shuffle=True, batch_size=config.batch_size,\n",
        "                                 num_workers=config.num_workers, collate_fn=train_dset.collate_fn)\n",
        "\n",
        "    if config.custom_train:\n",
        "        if config.load_checkpoint:\n",
        "            print(f\"Resuming from checkpoint: {config.file_checkpoint}\")\n",
        "            model = load_model(model, config.file_checkpoint)\n",
        "            # load stats\n",
        "            with open(os.path.join('/content/drive/MyDrive/DriveLM_project/DriveLM',\n",
        "                                   'multi_frame_results', config.file_checkpoint, 'stats.json'), 'r') as f:\n",
        "                stats = json.load(f)\n",
        "            min_train_loss, min_val_loss, losses, val_losses, epochs_ran = (\n",
        "                stats['min train loss'], stats['min val loss'],\n",
        "                stats['losses'], stats['val losses'], stats['epochs']\n",
        "            )\n",
        "            timestr = config.file_checkpoint\n",
        "        else:\n",
        "            os.mkdir(os.path.join('/content/drive/MyDrive/DriveLM_project/DriveLM',\n",
        "                                  'multi_frame_results', timestr))\n",
        "\n",
        "        lr = stats['learning rate'] if config.load_checkpoint else config.learning_rate\n",
        "        min_train_loss, min_val_loss = custom_train(min_train_loss, min_val_loss, best_model, epochs_ran, lr)\n",
        "        best_model = load_model(DriveVLM(config).to(device), 'latest_model')\n",
        "        test_loss = val_model(test_dataloader, best_model)\n",
        "        save_experiment([min_train_loss, min_val_loss, test_loss])\n",
        "    else:\n",
        "        train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hnTO2tc4ASw"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    from IPython.display import HTML\n",
        "    import time\n",
        "    display(HTML(\"<script>window.scrollTo(0, document.body.scrollHeight);</script>\"))\n",
        "    time.sleep(60)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install peft\n",
        "!pip install pycocoevalcap\n",
        "!pip install pycocotools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-qNLD3ckuKj",
        "outputId": "7bd45564-1850-41cc-bb45-fa678faca474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cpu)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.9.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.7.14)\n",
            "Requirement already satisfied: pycocoevalcap in /usr/local/lib/python3.11/dist-packages (1.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap) (2.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (2.0.2)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " def collate_fn_test(self, batch):\n",
        "\n",
        "        q_texts, imgs, a_texts, img_paths = zip(*batch)\n",
        "\n",
        "        imgs = torch.stack(list(imgs), dim=0)\n",
        "        img_paths = list(img_paths)\n",
        "        encodings = self.tokenizer(q_texts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "        labels = self.tokenizer(a_texts, padding=True, return_tensors='pt').input_ids.to(device)\n",
        "\n",
        "        return q_texts, encodings, imgs, labels, img_paths"
      ],
      "metadata": {
        "id": "BUksz3eXk4mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VIT_HIDDEN_STATE = 768\n",
        "VIT_SEQ_LENGTH = 49\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "\n",
        "    print(\n",
        "        f\"Trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "class DriveVLMT5(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Make tokenizer and text model\n",
        "        if config.lm == 'T5-Base':\n",
        "            self.model = T5ForConditionalGeneration.from_pretrained('google-t5/t5-base')\n",
        "        else:\n",
        "            self.model = T5ForConditionalGeneration.from_pretrained('google-t5/t5-large')\n",
        "\n",
        "            # For quantization\n",
        "            loftq_config = LoftQConfig(loftq_bits=8)\n",
        "            # Create LoRA model\n",
        "            lora_config = LoraConfig(\n",
        "                r=64,\n",
        "                lora_alpha=32,\n",
        "                loftq_config=loftq_config,\n",
        "                lora_dropout=0.05,\n",
        "                bias='none',\n",
        "                target_modules=['q', 'v']\n",
        "            )\n",
        "            self.model = get_peft_model(self.model, lora_config)\n",
        "\n",
        "        hidden_size = self.model.config.d_model\n",
        "\n",
        "        print('Trainable Parameters for LM model:')\n",
        "        print_trainable_parameters(self.model)\n",
        "\n",
        "        # Create instance for multi-view processor\n",
        "        self.mvp = self.MultiViewProcessor(config.gpa_hidden_size, hidden_size, config.lm, freeze=True)\n",
        "\n",
        "    class MultiViewProcessor(nn.Module):\n",
        "\n",
        "        def __init__(self, gpa_hidden_size, hidden_size, lm, freeze=False):\n",
        "\n",
        "            super().__init__()\n",
        "\n",
        "            # Use ViT for image embeddings\n",
        "            self.img_model = vit_b_32(weights='DEFAULT')\n",
        "            self.lm = lm\n",
        "\n",
        "            # Modal embedding to distinguish between image and text\n",
        "            self.modal_embeddings = nn.Embedding(2, hidden_size)\n",
        "            self.modal_embeddings.weight.data.normal_(mean=0.0, std=0.02)\n",
        "\n",
        "            # If we are freezing the CLIP embeddings\n",
        "            if freeze:\n",
        "                for param in self.img_model.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "            # Set matrices based on MIVC paper\n",
        "            self.w = nn.Linear(in_features=gpa_hidden_size, out_features=1)\n",
        "            self.Z = nn.Sequential(\n",
        "                nn.Linear(in_features=VIT_HIDDEN_STATE * VIT_SEQ_LENGTH, out_features=gpa_hidden_size, bias=False),\n",
        "                nn.Tanh()\n",
        "            )\n",
        "            self.G = nn.Sequential(\n",
        "                nn.Linear(in_features=VIT_HIDDEN_STATE * VIT_SEQ_LENGTH, out_features=gpa_hidden_size, bias=False),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "            if self.lm != 'T5-Base':\n",
        "              self.img_projection_layer = nn.Linear(in_features=VIT_HIDDEN_STATE, out_features=hidden_size)\n",
        "\n",
        "\n",
        "        def gpa(self, img_embeddings):\n",
        "\n",
        "            \"\"\"\"\n",
        "            Calculates the gated-pooling attention score for the image embeddings\n",
        "            :param img_embeddings: (6x768) dimensional\n",
        "            :return single embedding of size (768,)\n",
        "            \"\"\"\n",
        "\n",
        "            # Get weights for gated pooling attention\n",
        "            gpa_weights = torch.softmax(self.w(self.Z(img_embeddings) * self.G(img_embeddings)), dim=0  )\n",
        "\n",
        "            # Take a linear combination of all the image embeddings\n",
        "            fused_embeddings = torch.sum(gpa_weights * img_embeddings, dim=0)\n",
        "\n",
        "            return fused_embeddings\n",
        "\n",
        "        def get_img_embedding(self, imgs):\n",
        "\n",
        "            N = imgs.shape[0]\n",
        "\n",
        "            # Process into patches (N x 6 x 49 x H)\n",
        "            merged_embedding = torch.stack([self.img_model._process_input(img) for img in imgs], dim=0)\n",
        "\n",
        "            # Concatenate the batch class tokens -> (N, 6, 50, H)\n",
        "            batch_class_tokens = self.img_model.class_token.expand(merged_embedding.shape[1], -1, -1).repeat(N, 1, 1, 1)\n",
        "            merged_embedding = torch.cat([batch_class_tokens, merged_embedding], dim=2)\n",
        "\n",
        "            # Add positional embeddings and remove class token -> (N, 6, 49, H)\n",
        "            merged_embedding += self.img_model.encoder.pos_embedding.repeat(N, 1, 1, 1)\n",
        "            merged_embedding = merged_embedding[:, :, 1:]\n",
        "\n",
        "            # Get merged embedding and reshape to 2D embedding -> (N, 1, 49, H)\n",
        "            merged_embedding = torch.stack([self.gpa(embedding.flatten(start_dim=1)).reshape(VIT_SEQ_LENGTH,\n",
        "                                            VIT_HIDDEN_STATE) for embedding in merged_embedding], dim=0)\n",
        "\n",
        "            # Project to VL dimension -> (1, 49, H) (H is 512 for t5-small, 768 for t5-base)\n",
        "            if self.lm != 'T5-Base':\n",
        "              merged_embedding = self.img_projection_layer(merged_embedding)\n",
        "\n",
        "            # Add modal type embedding to merged embedding\n",
        "            merged_embedding += self.modal_embeddings(\n",
        "                torch.ones((1, merged_embedding.shape[1]), dtype=torch.int, device=device))\n",
        "\n",
        "            return merged_embedding\n",
        "\n",
        "        def forward(self, text_enc, imgs, text_model):\n",
        "\n",
        "            # Get the image embeddings (N x 1 x 49 x H)\n",
        "            imgs_embedding = self.get_img_embedding(imgs)\n",
        "\n",
        "            # Get the text embeddings (N x S x H)\n",
        "            text_embeddings = text_model.get_input_embeddings()(text_enc)\n",
        "\n",
        "            # Add modal embeddings to text\n",
        "            text_embeddings += self.modal_embeddings(torch.zeros((1, text_embeddings.shape[1]), dtype=torch.int,\n",
        "                                                                 device=device))\n",
        "\n",
        "            # Concatenate embeddings -> (1 x S x 512)\n",
        "            merged_embedding = torch.cat([text_embeddings, imgs_embedding], dim=1)\n",
        "\n",
        "            return merged_embedding\n",
        "\n",
        "    def forward(self, text_enc, imgs, labels=None):\n",
        "\n",
        "        # Get the merged embeddings\n",
        "        merged_embedding = self.mvp(text_enc, imgs, self.model)\n",
        "\n",
        "        # If training include the labels\n",
        "        return self.model(inputs_embeds=merged_embedding, labels=labels)\n",
        "\n",
        "    def generate(self, text_enc, imgs, lidar=None):\n",
        "\n",
        "        merged_embedding = self.mvp(text_enc, imgs, self.model)\n",
        "\n",
        "        attention_mask = torch.ones(merged_embedding.shape[:2], dtype=torch.long, device=device)\n",
        "        decoder_input_ids = torch.ones((merged_embedding.shape[0], 1), dtype=torch.long, device=device)*self.model.config.decoder_start_token_id\n",
        "        output_ids = self.model.generate(attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, inputs_embeds=merged_embedding, max_length=512, early_stopping=True)\n",
        "\n",
        "        return output_ids"
      ],
      "metadata": {
        "id": "ten1Mg_9k9ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Config = namedtuple('Instance', ['batch_size', 'gpa_hidden_size', 'model_name', 'lm'])\n",
        "\n",
        "config = Config(\n",
        "    batch_size = 16,\n",
        "    gpa_hidden_size = 128,\n",
        "    model_name = 'T5-Large-Q',\n",
        "    lm = 'T5-Large'\n",
        ")"
      ],
      "metadata": {
        "id": "f4VsP9IIlJdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# List all subfolders in your multi_frame_results\n",
        "glob.glob(\"/content/drive/MyDrive/DriveLM_project/DriveLM/multi_frame_results/*\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PnSfABrqjMi",
        "outputId": "4616b1b5-e06e-41bc-d0df-0fceeee40b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/DriveLM_project/DriveLM/multi_frame_results/20250804-055948',\n",
              " '/content/drive/MyDrive/DriveLM_project/DriveLM/multi_frame_results/20250804-060259',\n",
              " '/content/drive/MyDrive/DriveLM_project/DriveLM/multi_frame_results/20250804-061435',\n",
              " '/content/drive/MyDrive/DriveLM_project/DriveLM/multi_frame_results/20250804-063559',\n",
              " '/content/drive/MyDrive/DriveLM_project/DriveLM/multi_frame_results/20250804-063955']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "glob.glob(\"/content/drive/MyDrive/DriveLM_project/DriveLM/multi_frame_results/20250804-055948/*\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKctkgMtqz57",
        "outputId": "0d63ebf1-6a5e-442f-9b6e-fff74afa9688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l \"/content/drive/MyDrive/DriveLM_project/DriveLM/multi_frame_results/20250804-063955\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNQ3zY4ircLc",
        "outputId": "71d4bd49-555f-49a9-837a-d86403ece3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from tqdm import tqdm as progress_bar\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import T5Tokenizer\n",
        "\n",
        "# ===== Your trained run folder =====\n",
        "checkpoint_run = \"/content/drive/MyDrive/DriveLM_project/DriveLM/multi_frame_results/20250804-063955\"\n",
        "\n",
        "# ===== Paths =====\n",
        "base_dir = \"/content/drive/MyDrive/DriveLM_project/DriveLM\"\n",
        "ckpt_dir = os.path.join(base_dir, \"multi_frame_results\", checkpoint_run)\n",
        "\n",
        "# ===== Load processors and model =====\n",
        "model = DriveVLMT5(config).to(device)\n",
        "\n",
        "if config.lm == 'T5-Base':\n",
        "    processor = T5Tokenizer.from_pretrained('google-t5/t5-base')\n",
        "else:\n",
        "    processor = T5Tokenizer.from_pretrained('google-t5/t5-large')\n",
        "\n",
        "processor.add_tokens('<')\n",
        "\n",
        "# Load trained checkpoint\n",
        "model.load_state_dict(torch.load(os.path.join(ckpt_dir, 'latest_model.pth'), map_location=device))\n",
        "\n",
        "# ===== Dataset & DataLoader =====\n",
        "test_dset = MultiFrameDataset(\n",
        "    input_file=os.path.join(base_dir, \"data\", \"multi_frame\", \"prompts_for_drivelm.json\"),\n",
        "    tokenizer=processor,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))\n",
        "    ])\n",
        ")\n",
        "test_dloader = DataLoader(test_dset, shuffle=True, batch_size=1, drop_last=True, collate_fn=test_dset.collate_fn_test)\n",
        "\n",
        "# ===== Image ID mapping =====\n",
        "with open(os.path.join(base_dir, \"data\", \"multi_frame\", \"image_id.json\")) as f:\n",
        "    image_id_dict = json.load(f)\n",
        "\n",
        "# ===== Inference function =====\n",
        "def val_model(dloader):\n",
        "    model.eval()\n",
        "    test_data = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (q_texts, encodings, imgs, labels, img_paths) in progress_bar(enumerate(dloader), total=len(dloader)):\n",
        "            outputs = model.generate(encodings, imgs)\n",
        "            text_outputs = [processor.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "            for image_path, q_text, text_output in zip(img_paths, q_texts, text_outputs):\n",
        "                img_key = image_path[0]\n",
        "                test_data.append({'ImgID and Question': f\"{img_key} , {q_text}\", 'caption': text_output})\n",
        "\n",
        "                fig, axes = plt.subplots(3, 2, figsize=(12, 8))\n",
        "                for ax, img_p in zip(axes.flat, img_paths[0]):\n",
        "                    img = mpimg.imread(img_p)\n",
        "                    ax.imshow(img)\n",
        "                    ax.set_title(os.path.basename(img_p))\n",
        "                    ax.axis('off')\n",
        "\n",
        "                fig.suptitle(f\"Q: {q_text}\\nA: {text_output}\", fontsize=14)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "    # Save predictions\n",
        "    out_path = os.path.join(ckpt_dir, 'predictions-test.json')\n",
        "    with open(out_path, 'w') as f:\n",
        "        json.dump(test_data, f, indent=2)\n",
        "    print(f\"‚úÖ Predictions saved to {out_path}\")\n",
        "\n",
        "# ===== Run inference =====\n",
        "val_model(test_dloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "HPnt87HrvXuL",
        "outputId": "8ec3ba8e-9315-4249-ba68-ba17dc791b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable Parameters for LM model:\n",
            "Trainable params: 18874368 || all params: 756542464 || trainable%: 2.494819378704432\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/DriveLM_project/DriveLM/multi_frame_results/20250804-063955/latest_model.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2104445505.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Load trained checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latest_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# ===== Dataset & DataLoader =====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/DriveLM_project/DriveLM/multi_frame_results/20250804-063955/latest_model.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WfcBT6hdvYrA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1T6WN2op7J3J_9ZffnGl8L88DhljnoiGM",
      "authorship_tag": "ABX9TyPbmJH8x3kjjq2VnG3Mavu+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}